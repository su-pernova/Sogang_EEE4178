# -*- coding: utf-8 -*-
"""HW1_20181202.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sE1NU-xIh-UNlhqnPOy_F-323Y1Am6Ae

인공지능개론 #HW1 / 20181202 김수미
=======
간단한 XOR Table을 학습하는 NN 을 구성하는 문제입니다.

-  1-Layer, 2-Layer model을 각각 구성하여 XOR 결과를 비교합니다.
- 1-Layer, 2-Layer의 model을 Back propagation을 이용하여 학습시킵니다.
-  주어진 양식을 활용해 주시며, scale, 차원의 순서, hyper parameter등은 결과가 잘 나오는 방향으로 Tuning하셔도 무방합니다.
-  Layer의 Activation 함수 Sigmoid는 5번째 셀(cell)의 함수를 사용하시면 됩니다.
-  결과 재현을 위해 Weight,bias 값을 저장하여 함께 첨부해 주시기 바랍니다.
-  각 모델에서 loss 그래프와 testing step을 첨부하여 간단하게 자유 양식 결과 보고서(2~3장 내외)로 작성해 주세요.


* python으로 코드를 작성하는 경우, 양식에서 활용하는 라이브러리 외에 추가로 import 하여 사용하실 수 없습니다.
"""

# Commented out IPython magic to ensure Python compatibility.
# 이 외에 추가 라이브러리 사용 금지
import numpy as np
import random
import matplotlib.pyplot as plt
# %matplotlib inline

# Hyper parameters
# 학습의 횟수와 Gradient update에 쓰이는 learning rate입니다.
# 다른 값을 사용하여도 무방합니다.
epochs = 10000
learning_rate = 0.05

# Input data setting
# XOR data 
# 입력 데이터들, XOR Table 에 맞게 정의되어 있습니다.
X = np.array([0, 0, 1, 1, 0, 1, 0, 1]).reshape(2,4)
Y = np.array([0, 1, 1, 0]).reshape(1,4)

# 학습에 사용되는 Weigth 들의 초기값을 선언해 줍니다. 다른 값을 사용하여도 무방합니다.
# Weight Setting(2-layer)
W1 = np.random.randn(2, 3)
W2 = np.random.randn(3, 1)
B1 = np.random.randn(3, 1)
B2 = np.random.randn(1, 1)

# Weight Setting(1-layer)
W0 = np.random.randn(2, 1)
B0 = np.random.randn(1, 1)

##-----------------------------------##
##------- Activation Function -------##
##-----------------------------------##

def sigmoid(x):          
    return 1 / (np.exp(-x)+1)

##-----------------------------------##
##---------- Cost Function ----------##
##-----------------------------------##

def BCE(y, y_hat):
  return ( (-1) * (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) )

"""##1) 2-layer 모델을 통한 XOR table 학습
1개의 hidden layer 를 가지는 2-layer 모델을 통한 XOR table 학습과정 및 결과에 대한 내용입니다.
"""

# ----------------------------------- #
# ------ Training Step / 2-layer ------ #
# ----------------------------------- #
# 학습이 시작됩니다. epoch 사이즈만큼 for 문을 통해 학습됩니다.

errors = []
for epoch in range(epochs):

  # Layer에 맞는 Forward Network 구성
  Z1 = np.dot(W1.T, X) + B1
  x1  = sigmoid(Z1)

  Z2 = np.dot(W2.T, x1) + B2
  x2 = sigmoid(Z2)

  # Binary Corss Entropy(BCE)로 loss 계산
  loss = BCE(Y, x2)
  loss = loss[0][0]

  # Back propagation을 통한 Weight의 Gradient update 
  dx1  = np.dot(W2, x2-Y)
  dZ1 = dx1 * x1 * (1-x1)

  delta_W1 = np.dot(X, dZ1.T)
  delta_W2 = np.dot(x1, (x2-Y).T)
  delta_B1 = 1/4 * np.sum(dZ1, axis=1, keepdims=True)
  delta_B2 = 1/4 * np.sum(x2-Y, axis=1, keepdims=True)

  # 각 weight의 update 반영
  W1 = W1 -learning_rate * delta_W1
  W2 = W2 - learning_rate * delta_W2
  B1 = B1 - learning_rate * delta_B1
  B2 = B2 - learning_rate * delta_B2

        
        
  # 500번째 epoch마다 loss를 프린트
  if epoch%500 == 0:
    print("epoch[{}/{}] loss: {:.4f}".format(epoch,epochs,float(loss)))
        
  errors.append(loss)

# 학습이 끝난 후, loss를 확인합니다.
loss =  np.array(errors)
plt.plot(loss.reshape(epochs))
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

#-----------------------------------#
#--------- Testing Step ------------#
#-----------------------------------#

train_inp = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])
train_out = np.array([0, 1, 1, 0])

for idx in range(4):
    xin = train_inp[idx]
    ans = train_out[idx]
    
    x1 = sigmoid(np.matmul(xin,W1) + np.transpose(B1))
    x2 = sigmoid(np.matmul(x1,W2) + B2)

    pred = x2
    
    print("input: ", xin, ", answer: ", ans, ", pred: {:.4f}".format(float(pred)))

"""##2) 1-layer 모델을 통한 XOR table 학습
hidden layer 없이 오직 input layer 와 outpur layer 만을 가지는 1-layer 모델을 통한 XOR table 학습과정 및 결과에 대한 내용입니다.
"""

# ----------------------------------- #
# ------ Training Step / 1-layer ------ #
# ----------------------------------- #
# 학습이 시작됩니다. epoch 사이즈만큼 for 문을 통해 학습됩니다.

errors = []
for epoch in range(epochs):

  # Layer에 맞는 Forward Network 구성
  Z1 = np.dot(W0.T, X) + B0
  x2  = sigmoid(Z1)

  # Binary Corss Entropy(BCE)로 loss 계산
  loss = BCE(Y, x2)
  loss = loss[0][0]

  # Back propagation을 통한 Weight의 Gradient update 
  delta_W0 = np.dot(X, (x2-Y).T)
  delta_B0 = 1/4 * np.sum(x2-Y, axis=1, keepdims=True)

  # 각 weight의 update 반영
  W0 = W0 -learning_rate * delta_W0
  B0 = B0 - learning_rate * delta_B0
        
  # 500번째 epoch마다 loss를 프린트
  if epoch%500 == 0:
    print("epoch[{}/{}] loss: {:.4f}".format(epoch,epochs,float(loss)))
        
  errors.append(loss)

# 학습이 끝난 후, loss를 확인합니다.
loss =  np.array(errors)
plt.plot(loss.reshape(epochs))
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

#-----------------------------------#
#--------- Testing Step ------------#
#-----------------------------------#

train_inp = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])
train_out = np.array([0, 1, 1, 0])

for idx in range(4):
    xin = train_inp[idx]
    ans = train_out[idx]
    
    x1 = sigmoid(np.matmul(xin,W0) + (B0))

    pred = x1
    
    print("input: ", xin, ", answer: ", ans, ", pred: {:.4f}".format(float(pred)))

#-----------------------------------#
#--------- Weight Saving -----------#
#-----------------------------------#

# weight, bias를 저장하는 부분입니다.
# 학번에 자신의 학번으로 대체해 주세요.

#layer 1개인 경우
np.savetxt("20181202_layer1_weight.txt",(W0, B0),fmt="%s")

#layer 2개인 경우
np.savetxt("20181202_layer2_weight.txt",(W1, W2, B1, B2),fmt="%s")